{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Plutobi/Former/blob/main/Image_Report_Assist_Model_1_5_4b_it.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers"
      ],
      "metadata": {
        "id": "eF9Bz9VF4LGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local Inference on GPU\n",
        "Model page: https://huggingface.co/google/medgemma-1.5-4b-it\n",
        "\n",
        "‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/google/medgemma-1.5-4b-it)\n",
        "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè"
      ],
      "metadata": {
        "id": "Q3e1rCax4LGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model you are trying to use is gated. Please make sure you have access to it by visiting the model page.To run inference, either set HF_TOKEN in your environment variables/ Secrets or run the following cell to login. ü§ó"
      ],
      "metadata": {
        "id": "4QHtZ4104LGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "-hkXs-S84LGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"image-text-to-text\", model=\"google/medgemma-1.5-4b-it\")\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},\n",
        "            {\"type\": \"text\", \"text\": \"What animal is on the candy?\"}\n",
        "        ]\n",
        "    },\n",
        "]\n",
        "pipe(text=messages)"
      ],
      "metadata": {
        "id": "yGabmPJ64LGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"google/medgemma-1.5-4b-it\")\n",
        "model = AutoModelForImageTextToText.from_pretrained(\"google/medgemma-1.5-4b-it\")\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},\n",
        "            {\"type\": \"text\", \"text\": \"What animal is on the candy?\"}\n",
        "        ]\n",
        "    },\n",
        "]\n",
        "inputs = processor.apply_chat_template(\n",
        "\tmessages,\n",
        "\tadd_generation_prompt=True,\n",
        "\ttokenize=True,\n",
        "\treturn_dict=True,\n",
        "\treturn_tensors=\"pt\",\n",
        ").to(model.device)\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=40)\n",
        "print(processor.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
      ],
      "metadata": {
        "id": "kO7VePgu4LGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from PIL import Image\n",
        "\n",
        "uploaded = files.upload()\n",
        "image_filename = list(uploaded.keys())[0]\n",
        "image = Image.open(image_filename)\n",
        "image"
      ],
      "metadata": {
        "id": "XlIvQpsWEsMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadata = {\n",
        "    \"modality\": \"MRI\",\n",
        "    \"anatomical_region\": \"Brain\",\n",
        "    \"view\": \"Axial\",\n",
        "    \"contrast\": \"Not specified\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "iwPir8jUE-S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "### Instruction:\n",
        "You are a radiology documentation assistant powered by a medical language model.\n",
        "\n",
        "Your task is to generate a structured, non-diagnostic imaging report draft based on study metadata.\n",
        "Do NOT interpret image content.\n",
        "Do NOT identify abnormalities or provide diagnoses.\n",
        "\n",
        "### Imaging Study Metadata:\n",
        "Modality: {metadata[\"modality\"]}\n",
        "Anatomical Region: {metadata[\"anatomical_region\"]}\n",
        "View: {metadata[\"view\"]}\n",
        "Contrast: {metadata[\"contrast\"]}\n",
        "\n",
        "### Required Output Format:\n",
        "Study Type:\n",
        "- Concise study name.\n",
        "\n",
        "Views Included:\n",
        "- Imaging views provided.\n",
        "\n",
        "Structured Description:\n",
        "- Brief, neutral description of the imaging study.\n",
        "\n",
        "Notes:\n",
        "- Include safety and clarification notes if applicable.\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "lCNCeW2jFGXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa6e121a"
      },
      "source": [
        "messages_for_generation = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"url\": image_filename},\n",
        "            {\"type\": \"text\", \"text\": prompt}\n",
        "        ]\n",
        "    },\n",
        "]\n",
        "\n",
        "inputs = processor.apply_chat_template(\n",
        "    messages_for_generation,\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=True,\n",
        "    return_dict=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(model.device)\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=150,\n",
        "    do_sample=True,\n",
        "    temperature=0.3,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.2,\n",
        "    eos_token_id=processor.tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "generated_tokens = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
        "\n",
        "result = processor.decode(\n",
        "    generated_tokens,\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pm1ZCeTbrP9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d209c165"
      },
      "source": [
        "# Save the generated report to a text file\n",
        "file_name = \"report.txt\"\n",
        "with open(file_name, \"w\") as f:\n",
        "    f.write(result)\n",
        "\n",
        "print(f\"Report saved to {file_name}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}